{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d40246a",
   "metadata": {},
   "source": [
    "# Categorical Data Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b962d91",
   "metadata": {},
   "source": [
    "### What is Categorical Data Encoding\n",
    "\n",
    "Categorical data is a type of data that represents categories or groups. Examples of categorical data include gender, color, or product type. In data exploration, categorical data encoding is the process of converting categorical data into numerical data that can be analyzed by machine learning algorithms.\n",
    "\n",
    "### Why do we Encode our data\n",
    "\n",
    "Machine learning algorithms cannot work with categorical data directly. They require numerical data to perform mathematical operations such as addition and multiplication. Therefore, categorical data encoding is necessary to convert categorical data into numerical data.\n",
    "\n",
    "![Encoding Example](assests/encoding-3.png \"Encoding Example\")\n",
    "\n",
    "In the above picture it can be seen that the *Qualitative* column of *Height* is being encoded into *Quantitative* representation. This is done by assigning a number for each unique entry in the *Height* column in the right table. Now each of the height values are being represented by a distinct number. This is just a simplistic way of encoding things.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a80bfb",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "One-hot encoding is a technique that creates a binary column for each category in a categorical variable. The value is 1 if the category is present and 0 if it is not.\n",
    "\n",
    "![One-Hot Encoding Example](assests/oneHotenc.png \"One-Hot Encoding Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bcbbba",
   "metadata": {},
   "source": [
    "### One-Hot Encoding in Pandas\n",
    "\n",
    "One-Hot encoding of the data can be done in Pandas by using the *get_dummies* function.\n",
    "Lets take a look how that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f318a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Team_A  Team_B  Team_C\n",
      "0       1       0       0\n",
      "1       1       0       0\n",
      "2       0       1       0\n",
      "3       0       1       0\n",
      "4       0       1       0\n",
      "5       0       1       0\n",
      "6       0       0       1\n",
      "7       0       0       1\n"
     ]
    }
   ],
   "source": [
    "# importing Pandas library\n",
    "import pandas as pd\n",
    "\n",
    "# creating a dictionary with data\n",
    "data = {\n",
    "    \"Team\": [\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\"]\n",
    "}\n",
    "\n",
    "# initializing a dataframe with the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# applying one-hot encoding by using get_dummies function\n",
    "df_encoded = pd.get_dummies(df, columns=['Team'])\n",
    "\n",
    "# printing out the encoded data frame\n",
    "print(df_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29eb0b",
   "metadata": {},
   "source": [
    "### Lets try \"One-Hot Encoding\" on a Dataset!\n",
    "\n",
    "For this we will be using the dataset called \"Iris Dataset\". This data sets consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length. The type of Iris will be the qualitative data that we will be applying encoding on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74bfae33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal.length  sepal.width  petal.length  petal.width variety\n",
      "0           5.1          3.5           1.4          0.2  Setosa\n",
      "1           4.9          3.0           1.4          0.2  Setosa\n",
      "2           4.7          3.2           1.3          0.2  Setosa\n",
      "3           4.6          3.1           1.5          0.2  Setosa\n",
      "4           5.0          3.6           1.4          0.2  Setosa\n"
     ]
    }
   ],
   "source": [
    "# loading Iris dataset\n",
    "iris_data = pd.read_csv(\"data/iris.csv\")\n",
    "\n",
    "# printing head\n",
    "print(iris_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae936f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Setosa\n",
      "1    Setosa\n",
      "2    Setosa\n",
      "3    Setosa\n",
      "4    Setosa\n",
      "Name: variety, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# extracting the qualitative data column which is the variety column\n",
    "iris_variety = iris_data[\"variety\"]\n",
    "\n",
    "print(iris_variety.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be12d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Setosa  Versicolor  Virginica\n",
      "9         1           0          0\n",
      "26        1           0          0\n",
      "33        1           0          0\n",
      "15        1           0          0\n",
      "141       0           0          1\n",
      "16        1           0          0\n",
      "50        0           1          0\n",
      "1         1           0          0\n",
      "37        1           0          0\n",
      "52        0           1          0\n"
     ]
    }
   ],
   "source": [
    "# applying one-hot encoding on the extracted column\n",
    "iris_variety_enc = pd.get_dummies(iris_variety)\n",
    "\n",
    "# sampling random rows from the encodings\n",
    "sample = iris_variety_enc.sample(n=10)\n",
    "\n",
    "# printing encoded values\n",
    "print(sample.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a2faf",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "Label encoding is a technique that assigns a unique integer value to each category in a categorical variable.\n",
    "\n",
    "<div>\n",
    "<img src=\"assests/labelEnc.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b65477",
   "metadata": {},
   "source": [
    "### Label Encoding in Pandas and SkLearn\n",
    "\n",
    "Label encoding of the data can be done in Pandas by using the *LabelEncoder* class that is available in SkLearn library.\n",
    "\n",
    "More about Sklearn's *LabelEncoder* class [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder)\n",
    "\n",
    "More about Sklearn [here](https://scikit-learn.org/stable/)\n",
    "\n",
    "Lets take a look how that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0414886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/ammar/.local/lib/python3.9/site-packages (1.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/ammar/.local/lib/python3.9/site-packages (from scikit-learn) (1.22.4)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ammar/.local/lib/python3.9/site-packages (from scikit-learn) (3.0.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/ammar/.local/lib/python3.9/site-packages (from scikit-learn) (1.7.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ammar/.local/lib/python3.9/site-packages (from scikit-learn) (1.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "# installing sklearn\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19212aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Team  Team Encoded\n",
      "0    A             0\n",
      "1    A             0\n",
      "2    B             1\n",
      "3    B             1\n",
      "4    B             1\n",
      "5    B             1\n",
      "6    C             2\n",
      "7    C             2\n"
     ]
    }
   ],
   "source": [
    "# importing the LabelEncoder from sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# creating a dictionary with data\n",
    "data = {\n",
    "    \"Team\": [\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\"]\n",
    "}\n",
    "\n",
    "# initializing a dataframe with the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing the LabelEncoder class object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# applying label encoding by using get_dummies function\n",
    "df['Team Encoded'] = label_encoder.fit_transform(df['Team'])\n",
    "\n",
    "# printing out the encoded data frame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334f9f93",
   "metadata": {},
   "source": [
    "### Lets try \"Label Encoding\" on Iris Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d51e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        variety  variety encoded\n",
      "84   Versicolor                1\n",
      "61   Versicolor                1\n",
      "20       Setosa                0\n",
      "88   Versicolor                1\n",
      "130   Virginica                2\n",
      "63   Versicolor                1\n",
      "18       Setosa                0\n",
      "139   Virginica                2\n",
      "115   Virginica                2\n",
      "37       Setosa                0\n"
     ]
    }
   ],
   "source": [
    "# applying label encoding on the variety column\n",
    "iris_data[\"variety encoded\"] = label_encoder.fit_transform(iris_data[\"variety\"])\n",
    "\n",
    "# sampling random rows from the encodings\n",
    "sample = iris_data.sample(n=10)\n",
    "\n",
    "# printing encoded values\n",
    "print(sample[[\"variety\", \"variety encoded\"]].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02dc6f3",
   "metadata": {},
   "source": [
    "## Binary Encoding\n",
    "\n",
    "Binary encoding is a technique that creates binary columns for each category in a categorical variable, similar to one-hot encoding. However, instead of creating a binary column for each category, binary encoding creates a binary column for each unique combination of categories.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"assests/binaryEnc.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e333e3ee",
   "metadata": {},
   "source": [
    "### Binary Encoding in Pandas and Category Encoder library\n",
    "\n",
    "Binary encoding of the data can be done in Pandas by using the *BinaryEncoder* class that is available in Category Encoder library.\n",
    "\n",
    "More about *Binary Encoder* libary [here](https://pypi.org/project/category-encoders/)\n",
    "\n",
    "Lets take a look how that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6985b7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: category-encoders in /home/ammar/.local/lib/python3.9/site-packages (2.6.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/ammar/.local/lib/python3.9/site-packages (from category-encoders) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/ammar/.local/lib/python3.9/site-packages (from category-encoders) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/ammar/.local/lib/python3.9/site-packages (from category-encoders) (1.7.3)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /home/ammar/.local/lib/python3.9/site-packages (from category-encoders) (0.5.6)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /home/ammar/.local/lib/python3.9/site-packages (from category-encoders) (1.3.5)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /home/ammar/.local/lib/python3.9/site-packages (from category-encoders) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ammar/.local/lib/python3.9/site-packages (from pandas>=1.0.5->category-encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=1.0.5->category-encoders) (2021.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ammar/.local/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ammar/.local/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (1.1.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/ammar/.local/lib/python3.9/site-packages (from statsmodels>=0.9.0->category-encoders) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ammar/.local/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category-encoders) (3.0.9)\n"
     ]
    }
   ],
   "source": [
    "# installing category encoder library\n",
    "!pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f0258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits_0  Fruits_1\n",
      "0         0         1\n",
      "1         1         0\n",
      "2         1         1\n"
     ]
    }
   ],
   "source": [
    "# importing the BinaryEncoder from category encoder\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "# creating a dictionary with data\n",
    "data = {\n",
    "    \"Fruits\": [\"Apple\",\"Banana\",\"Orange\"]\n",
    "}\n",
    "\n",
    "# initializing a dataframe with the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing the BinaryEncoder class object and passing the values \n",
    "binary_encoder = BinaryEncoder(cols=[\"Fruits\"])\n",
    "\n",
    "# applying binary encoding\n",
    "encoded_data = binary_encoder.fit_transform(df)\n",
    "\n",
    "# printing out the encoded data frame\n",
    "print(encoded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d70c8",
   "metadata": {},
   "source": [
    "### Lets try \"Binary Encoding\" on Iris Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab81883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal.length  sepal.width  petal.length  petal.width  variety_0  \\\n",
      "2             4.7          3.2           1.3          0.2          0   \n",
      "40            5.0          3.5           1.3          0.3          0   \n",
      "52            6.9          3.1           4.9          1.5          1   \n",
      "103           6.3          2.9           5.6          1.8          1   \n",
      "131           7.9          3.8           6.4          2.0          1   \n",
      "\n",
      "     variety_1  variety encoded  \n",
      "2            1                0  \n",
      "40           1                0  \n",
      "52           0                1  \n",
      "103          1                2  \n",
      "131          1                2  \n"
     ]
    }
   ],
   "source": [
    "# initializing the BinaryEncoder class object and passing the values \n",
    "binary_encoder = BinaryEncoder(cols=[\"variety\"])\n",
    "\n",
    "# applying binary encoding on the variety column\n",
    "iris_variety_enc = binary_encoder.fit_transform(iris_data, return_df=True)\n",
    "\n",
    "# sampling random rows from the encodings\n",
    "sample = iris_variety_enc.sample(n=10)\n",
    "\n",
    "# printing encoded values\n",
    "print(sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0783b8",
   "metadata": {},
   "source": [
    "## Count Encoding\n",
    "\n",
    "Count encoding is a technique that replaces each category with the count of its occurrences in the dataset. This method is particularly useful for categorical variables with a large number of categories that have a similar frequency of occurrence\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"assests/countEnc.png\" width=\"90%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af71408b",
   "metadata": {},
   "source": [
    "### Count Encoding in Pandas and Category Encoder library\n",
    "\n",
    "Label encoding of the data can be done in Pandas by using the *CountEncoder* class that is available in Category Encoder library.\n",
    "\n",
    "More about *Count Encoder* libary [here](https://pypi.org/project/category-encoders/)\n",
    "\n",
    "Lets take a look how that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8c845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fruits  Fruits Encoded\n",
      "0   Apple               2\n",
      "1  Banana               3\n",
      "2   Apple               2\n",
      "3  Banana               3\n",
      "4  Banana               3\n"
     ]
    }
   ],
   "source": [
    "# importing the CountEncoder from category encoder\n",
    "from category_encoders import CountEncoder\n",
    "\n",
    "# creating a dictionary with data\n",
    "data = {\n",
    "    \"Fruits\": [\"Apple\",\"Banana\",\"Apple\",\"Banana\",\"Banana\"]\n",
    "}\n",
    "\n",
    "# initializing a dataframe with the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing the CountEncoder class object\n",
    "count_encoder = CountEncoder()\n",
    "\n",
    "# applying count encoding\n",
    "df[\"Fruits Encoded\"] = count_encoder.fit_transform(df[\"Fruits\"])\n",
    "\n",
    "# printing out the data frame with count encodings\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c1c02",
   "metadata": {},
   "source": [
    "### Lets try \"Count Encoding\" on Iris Dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f48f1a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal.length  sepal.width  petal.length  petal.width     variety  \\\n",
      "81            5.5          2.4           3.7          1.0  Versicolor   \n",
      "1             4.9          3.0           1.4          0.2      Setosa   \n",
      "19            5.1          3.8           1.5          0.3      Setosa   \n",
      "32            5.2          4.1           1.5          0.1      Setosa   \n",
      "131           7.9          3.8           6.4          2.0   Virginica   \n",
      "\n",
      "     variety encoded  \n",
      "81                50  \n",
      "1                 50  \n",
      "19                50  \n",
      "32                50  \n",
      "131               50  \n"
     ]
    }
   ],
   "source": [
    "# initializing the CountEncoder class object and passing the values \n",
    "count_encoder = CountEncoder()\n",
    "\n",
    "# making a copy of the loaded iris DataFrame\n",
    "iris_data_count_df = iris_data.copy()\n",
    "\n",
    "# applying count encoding on the variety column\n",
    "iris_data_count_df[\"variety encoded\"] = count_encoder.fit_transform(iris_data_count_df[\"variety\"], return_df=True)\n",
    "\n",
    "# sampling random rows from the encodings\n",
    "sample = iris_data_count_df.sample(n=10)\n",
    "\n",
    "# printing encoded values\n",
    "print(sample.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b06247",
   "metadata": {},
   "source": [
    "**In the above output can you spot the problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84503a",
   "metadata": {},
   "source": [
    "## Target Encoding\n",
    "\n",
    "Target encoding is a technique that replaces each category in the feature with the mean of the target variable for that category. This method works well for nominal and ordinal data, but can lead to overfitting when the number of instances in a category is small.\n",
    "\n",
    "<br>\n",
    "<div>\n",
    "<img src=\"assests/targetEnc.png\" width=\"60%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dff224",
   "metadata": {},
   "source": [
    "### Target Encoding in Pandas and Category Encoder library\n",
    "\n",
    "Target encoding of the data can be done in Pandas by using the *TargetEncoder* class that is available in Category Encoder library.\n",
    "\n",
    "More about *Target Encoder* libary [here](https://pypi.org/project/category-encoders/)\n",
    "\n",
    "Lets take a look how that would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0203b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   feature_1 feature_2  target  feature_2 encoded\n",
      "0          1         A       1           0.759121\n",
      "1          2         A       0           0.759121\n",
      "2          3         A       1           0.759121\n",
      "3          4         A       1           0.759121\n",
      "4          5         A       1           0.759121\n",
      "5          6         B       1           0.737128\n",
      "6          7         B       0           0.737128\n",
      "7          8         B       1           0.737128\n"
     ]
    }
   ],
   "source": [
    "# importing the TargetEncoder from category encoder\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# creating a dictionary with data\n",
    "data = {\n",
    "    \"feature_1\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "    \"feature_2\": [\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\"],\n",
    "    \"target\": [1, 0, 1, 1, 1, 1, 0, 1],\n",
    "}\n",
    "\n",
    "# initializing a dataframe with the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# initializing the TargetEncoder class object\n",
    "target_encoder = TargetEncoder(cols=[\"feature_2\"])\n",
    "\n",
    "# applying target encoding\n",
    "df[\"feature_2 encoded\"] = target_encoder.fit_transform(df[\"feature_2\"], df['target'])\n",
    "\n",
    "# printing out the data frame with target encodings\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf58d6",
   "metadata": {},
   "source": [
    "### You know the drill, try it on Iris dataset?\n",
    "### Discuss whether we can truly apply Target Encoding on Iris dataset or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b970071",
   "metadata": {},
   "source": [
    "# Data Normalization and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d45558c",
   "metadata": {},
   "source": [
    "### What is normalization and scaling?\n",
    "\n",
    "Data normalization and scaling are important preprocessing steps in data exploration and analysis. The purpose of normalization and scaling is to bring all features to the same scale or range, so that they can be compared and analyzed meaningfully.\n",
    "\n",
    "### Why do we normalize and scale data?\n",
    "\n",
    "1. Improving accuracy annd stability of mamchine learning models\n",
    "2. Facilitating convergence of optimization algorithms\n",
    "3. Improving interpretability of data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1971b58",
   "metadata": {},
   "source": [
    "## Min-Max Scaling\n",
    "\n",
    "Min-max scaling, also known as normalization, scales the data to a fixed range, usually between 0 and 1. The formula for min-max scaling is:\n",
    "\n",
    "```\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "```\n",
    "Where X is a feature value, X_min is the minimum value of that feature, and X_max is the maximum value of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616133ed",
   "metadata": {},
   "source": [
    "### Min-Max Scaler in Pandas using Sklearn\n",
    "\n",
    "In order to get functionality to apply scaling on the DataFrame we will using the *preprocessing* module of Sklearn.\n",
    "\n",
    "Read more about Sklearn's preprocessing module [here](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "More on MinMaxScaler [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e84253b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A    B\n",
      "0   1   10\n",
      "1   2   20\n",
      "2   3   30\n",
      "3   4   40\n",
      "4   5   50\n",
      "5   6   60\n",
      "6   7   70\n",
      "7   8   80\n",
      "8   9   90\n",
      "9  10  100\n"
     ]
    }
   ],
   "source": [
    "# importing the MinMax scaler from Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# creating a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'B': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "})\n",
    "\n",
    "# printing the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6685d4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: <class 'numpy.ndarray'>\n",
      "          A         B\n",
      "0  0.000000  0.000000\n",
      "1  0.111111  0.111111\n",
      "2  0.222222  0.222222\n",
      "3  0.333333  0.333333\n",
      "4  0.444444  0.444444\n",
      "5  0.555556  0.555556\n",
      "6  0.666667  0.666667\n",
      "7  0.777778  0.777778\n",
      "8  0.888889  0.888889\n",
      "9  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# create a MinMaxScaler object\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_scaled = min_max_scaler.fit_transform(df)\n",
    "\n",
    "# datatype of df_scaled\n",
    "print(\"Data Type: {}\".format(type(df_scaled)))\n",
    "\n",
    "# create a new DataFrame with the normalized data\n",
    "normalized_df = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "# printing the \n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d8ade1",
   "metadata": {},
   "source": [
    "*What if we add outliers to the above data sample and see what the results are? Is the normalization acceptable or not?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e33e9e",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "Standardization scales the data so that it has a mean of 0 and a standard deviation of 1. The formula for standardization is:\n",
    "\n",
    "```\n",
    "X_scaled = (X - mean(X)) / std(X)\n",
    "```\n",
    "Where X is a feature value, mean(X) is the mean value of that feature, and std(X) is the standard deviation of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73abe636",
   "metadata": {},
   "source": [
    "### Standardization in Pandas using Sklearn\n",
    "\n",
    "In order to get functionality to apply scaling on the DataFrame we will using the *preprocessing* module of Sklearn.\n",
    "\n",
    "Read more about Sklearn's preprocessing module [here](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "More on StandardScaler [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9427065e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A    B\n",
      "0   1   10\n",
      "1   2   20\n",
      "2   3   30\n",
      "3   4   40\n",
      "4   5   50\n",
      "5   6   60\n",
      "6   7   70\n",
      "7   8   80\n",
      "8   9   90\n",
      "9  10  100\n"
     ]
    }
   ],
   "source": [
    "# importing the StandardScaler scaler from Sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# creating a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'B': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "})\n",
    "\n",
    "# printing the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62315add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: <class 'numpy.ndarray'>\n",
      "          A         B\n",
      "0 -1.566699 -1.566699\n",
      "1 -1.218544 -1.218544\n",
      "2 -0.870388 -0.870388\n",
      "3 -0.522233 -0.522233\n",
      "4 -0.174078 -0.174078\n",
      "5  0.174078  0.174078\n",
      "6  0.522233  0.522233\n",
      "7  0.870388  0.870388\n",
      "8  1.218544  1.218544\n",
      "9  1.566699  1.566699\n"
     ]
    }
   ],
   "source": [
    "# create a StandardScaler object\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_scaled = standard_scaler.fit_transform(df)\n",
    "\n",
    "# datatype of df_scaled\n",
    "print(\"Data Type: {}\".format(type(df_scaled)))\n",
    "\n",
    "# create a new DataFrame with the normalized data\n",
    "normalized_df = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "# printing the \n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea2455",
   "metadata": {},
   "source": [
    "*Lets check manually if the mean of the columns equal to 0*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7f53ae",
   "metadata": {},
   "source": [
    "## Robust Scaling\n",
    "\n",
    "Robust scaling is similar to standardization, but it uses the median and interquartile range instead of the mean and standard deviation. The formula for robust scaling is:\n",
    "\n",
    "```\n",
    "X_scaled = (X - median(X)) / IQR(X)\n",
    "```\n",
    "where X is a feature value, median(X) is the median value of that feature, and IQR(X) is the interquartile range of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107aec9b",
   "metadata": {},
   "source": [
    "### Robust Scaling in Pandas using Sklearn\n",
    "\n",
    "In order to get functionality to apply scaling on the DataFrame we will using the *preprocessing* module of Sklearn.\n",
    "\n",
    "Read more about Sklearn's preprocessing module [here](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "More on RobustScaler [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html#sklearn.preprocessing.RobustScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33bb4254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A    B\n",
      "0   1   10\n",
      "1   2   20\n",
      "2   3   30\n",
      "3   4   40\n",
      "4   5   50\n",
      "5   6   60\n",
      "6   7   70\n",
      "7   8   80\n",
      "8   9   90\n",
      "9  10  100\n"
     ]
    }
   ],
   "source": [
    "# importing the RobustScaler scaler from Sklearn\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# creating a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'B': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "})\n",
    "\n",
    "# printing the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3983667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: <class 'numpy.ndarray'>\n",
      "          A         B\n",
      "0 -1.000000 -1.000000\n",
      "1 -0.777778 -0.777778\n",
      "2 -0.555556 -0.555556\n",
      "3 -0.333333 -0.333333\n",
      "4 -0.111111 -0.111111\n",
      "5  0.111111  0.111111\n",
      "6  0.333333  0.333333\n",
      "7  0.555556  0.555556\n",
      "8  0.777778  0.777778\n",
      "9  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# create a RobustScaler object\n",
    "robust_scaler = RobustScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_scaled = robust_scaler.fit_transform(df)\n",
    "\n",
    "# datatype of df_scaled\n",
    "print(\"Data Type: {}\".format(type(df_scaled)))\n",
    "\n",
    "# create a new DataFrame with the normalized data\n",
    "normalized_df = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "# printing the \n",
    "print(normalized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeeff19",
   "metadata": {},
   "source": [
    "## Max Abs Scaling\n",
    "\n",
    "Max Abs scaling scales the data to the absolute maximum value of each feature. The formula for max abs scaling is:\n",
    "\n",
    "```\n",
    "X_scaled = X / max(abs(X))\n",
    "```\n",
    "where X is a feature value.\n",
    "\n",
    "*What is the purpose of abs (absolute) in this scaling technique, what does this operation adds to the process?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1f880",
   "metadata": {},
   "source": [
    "### Max Abs Scaling in Pandas using Sklearn\n",
    "\n",
    "In order to get functionality to apply scaling on the DataFrame we will using the *preprocessing* module of Sklearn.\n",
    "\n",
    "Read more about Sklearn's preprocessing module [here](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "More on RobustScaler [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html#sklearn.preprocessing.MaxAbsScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8384dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    A    B\n",
      "0   1   10\n",
      "1   2   20\n",
      "2   3   30\n",
      "3   4   40\n",
      "4   5   50\n",
      "5   6   60\n",
      "6   7   70\n",
      "7   8   80\n",
      "8   9   90\n",
      "9  10  100\n"
     ]
    }
   ],
   "source": [
    "# importing the MaxAbsScaler scaler from Sklearn\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "# creating a dataframe\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'B': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "})\n",
    "\n",
    "# printing the dataframe\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8ea7b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type: <class 'numpy.ndarray'>\n",
      "     A    B\n",
      "0  0.1  0.1\n",
      "1  0.2  0.2\n",
      "2  0.3  0.3\n",
      "3  0.4  0.4\n",
      "4  0.5  0.5\n",
      "5  0.6  0.6\n",
      "6  0.7  0.7\n",
      "7  0.8  0.8\n",
      "8  0.9  0.9\n",
      "9  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# create a MaxAbsScaler object\n",
    "max_abs_scaler = MaxAbsScaler()\n",
    "\n",
    "# fit and transform the data\n",
    "df_scaled = max_abs_scaler.fit_transform(df)\n",
    "\n",
    "# datatype of df_scaled\n",
    "print(\"Data Type: {}\".format(type(df_scaled)))\n",
    "\n",
    "# create a new DataFrame with the normalized data\n",
    "normalized_df = pd.DataFrame(df_scaled, columns=df.columns)\n",
    "\n",
    "# printing the \n",
    "print(normalized_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
